import time
import csv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options

# ==========================================
# âš™ï¸ ì „ëµ ìˆ˜ì •: ê¸°ì¤€ ì™„í™” & ì „êµ­ í™•ì¥
# ==========================================
TARGET_MAX_RATING = 3.9  # ê¸°ì¤€ì„ 3.9ë¡œ ì˜¬ë ¤ì„œ ë¦¬ë·° ë§ì€ í˜¸í…”ë„ í¬í•¨ì‹œí‚´
# ì¼ë³¸ ì£¼ìš” ë„ì‹œ + ë¶€ì • ë¦¬ë·°ê°€ ë‚˜ì˜¤ê¸° ì‰¬ìš´ í‚¤ì›Œë“œ ì¡°í•©
SEARCH_KEYWORDS = [
    "Tokyo Capsule Hotel", "Tokyo Cheap Hotel", "Tokyo Hostel",
    "Osaka Capsule Hotel", "Osaka Cheap Hotel", "Osaka Hostel",
    "Fukuoka Capsule Hotel", "Fukuoka Business Hotel",
    "Kyoto Guesthouse", "Kyoto Cheap Hotel",
    "Sapporo Hotel", "Okinawa Hotel",
    "Nagoya Business Hotel"
]
# ==========================================

options = Options()
options.add_argument("--lang=en")
driver = webdriver.Chrome(options=options)

collected_urls = set()

print(f"ğŸ¯ ì „ëµ: ì¼ë³¸ ì „êµ­ ëŒ€ìƒ / í‰ì  {TARGET_MAX_RATING}ì  ì´í•˜ / ê°€ì„±ë¹„ ìˆ™ì†Œ ìœ„ì£¼ ìˆ˜ì§‘")

for keyword in SEARCH_KEYWORDS:
    # ê²€ìƒ‰ì–´ URL ìƒì„± (êµ¬ê¸€ë§µ ê²€ìƒ‰ ì¿¼ë¦¬)
    search_url = f"https://www.google.com/maps/search/{keyword.replace(' ', '+')}"

    print(f"\nğŸ” ê²€ìƒ‰ ì¤‘: '{keyword}' ...")
    driver.get(search_url)
    time.sleep(5)

    try:
        scrollable_div = driver.find_element(By.CSS_SELECTOR, "div[role='feed']")
    except:
        print("âš ï¸ ê²°ê³¼ ì—†ìŒ í˜¹ì€ ë¡œë”© ì‹¤íŒ¨. ë‹¤ìŒ í‚¤ì›Œë“œë¡œ.")
        continue

    # í‚¤ì›Œë“œ í•˜ë‚˜ë‹¹ ìŠ¤í¬ë¡¤ 30ë²ˆ (ì¶©ë¶„íˆ ë§ì´)
    for i in range(30):
        cards = driver.find_elements(By.CSS_SELECTOR, "div.Nv2PK")

        for card in cards:
            try:
                # URL ì¶”ì¶œ
                link_elem = card.find_element(By.TAG_NAME, "a")
                url = link_elem.get_attribute("href")
                if not url or "/maps/place/" not in url: continue
                clean_url = url.split("?")[0]

                if clean_url in collected_urls: continue

                # í‰ì  í™•ì¸
                try:
                    score_text = card.find_element(By.CSS_SELECTOR, "span.MW4etd").text
                    score = float(score_text)
                except:
                    score = 0.0  # í‰ì  ì—†ìœ¼ë©´ ì‹ ê·œ í˜¸í…”ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì¼ë‹¨ ìˆ˜ì§‘

                # 4.2ì  ì´í•˜ë©´ ìˆ˜ì§‘ (ê¸°ì¤€ ì™„í™”)
                if score <= TARGET_MAX_RATING:
                    collected_urls.add(clean_url)
                    # ë¡œê·¸ ì¤„ì´ê¸° (ë„ˆë¬´ ë§ì´ ëœ¨ë©´ ì •ì‹ ì—†ìŒ)
                    if len(collected_urls) % 10 == 0:
                        print(f"   Op.. í˜„ì¬ ì´ {len(collected_urls)}ê°œ URL í™•ë³´ ì¤‘")
            except:
                pass

        # ìŠ¤í¬ë¡¤
        driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", scrollable_div)
        time.sleep(1.5)

    print(f"   â¡ï¸ '{keyword}' ì™„ë£Œ. ëˆ„ì  URL: {len(collected_urls)}ê°œ")

# ì €ì¥
with open("data/raw/hotel_urls.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    for u in collected_urls:
        writer.writerow([u])

driver.quit()
print(f"\nğŸ”¥ [ìµœì¢… ì™„ë£Œ] ì´ {len(collected_urls)}ê°œì˜ í˜¸í…” URLì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
print("ğŸ‘‰ ì´ì œ crawler_50k_final.pyë¥¼ ì‹¤í–‰í•´ì„œ ë¦¬ë·°ë¥¼ ê¸ì–´ëª¨ìœ¼ì„¸ìš”!")
